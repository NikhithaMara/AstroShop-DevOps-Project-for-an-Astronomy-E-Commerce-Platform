Kubernetes Implementation for the project:

* I have created an EKS Cluster using Terraform. To interact with the cluster, I am using the kubectl command-line tool, which I have installed on an EC2 instance.
 When I run kubectl get nodes, it doesn’t show any nodes because kubectl doesn’t know which cluster it is connected to.

* To view clusters, we can use:
kubectl config view
* The context is the current cluster kubectl is connected to. To see the current context, use:
kubectl config current-context
This shows the cluster that kubectl is connected to.
* The cluster info is present in the kubeconfig file. To switch between different clusters, use:
kubectl config use-context <cluster-name>

* We need to update the kubeconfig file, which requires the AWS CLI.
I installed the AWS CLI following the AWS documentation and configured it using the Access Key and Secret Access Key found under Security Credentials for my IAM user on the AWS console (or created new credentials if needed).
* To update the kubeconfig file, use the following command:
aws eks update-kubeconfig --region <region-name> --name <cluster-name>
Example:
aws eks update-kubeconfig --region us-west-2 --name my-eks-cluster1

* Now, when we run:
kubectl get nodes
kubectl config current-context
it displays the nodes and the current cluster connected to.
Example output of current context:
arn:aws:eks:us-west-2:443370696298:cluster/my-eks-cluster1

Kubernetes Implementation Overview:
When we deploy applications as pods on Kubernetes, the following components are used:
*Service Account:
Just like users have accounts to communicate with AWS (configured via AWS CLI using access and secret keys), pods need a service account to interact with the Kubernetes cluster.
A service account has permissions to run within the cluster and namespace. If no service account is specified, the default account is assigned.
To assign permissions, create a role and bind it to the service account.
Use: kubectl get sa
to list service accounts.

* Deployment:
Containers are ephemeral, meaning they are short-lived. By default, if a container stops, it won’t restart unless a restart policy is set. This makes containers suitable mostly for local development.
Kubernetes Deployments solve this by managing pods with ReplicaSets, which ensure the desired number of pods are always running.
If a pod crashes, the deployment automatically creates a new one to maintain the desired state, providing auto-healing and autoscaling.

* Service:
For example, if a frontend microservice needs to talk to a backend pod, but the backend pod restarts and changes its IP, the frontend won’t be able to reach it by IP.
Kubernetes Services solve this problem by providing a stable DNS name. Services use labels and selectors to route traffic to the correct pods regardless of IP changes.
To expose an application externally, a service of type LoadBalancer can be created, which routes traffic to the frontend pods.

* Load Balancer:
ELB routes traffic to backend servers outside Kubernetes. To expose applications, services need to be of type LoadBalancer.
Having many LoadBalancer services can be costly, so sometimes alternatives like Ingress controllers are used.

* HPA and ReplicaSet:
ReplicaSets maintain a fixed number of pods. HPA adjusts the number of pods automatically based on metrics like CPU usage, updating the ReplicaSet accordingly.

* Kubernetes Manifests
Every Kubernetes resource is defined in YAML manifests. Each manifest usually has:
apiVersion: the Kubernetes API version in use
kind: resource type (ServiceAccount, Deployment, Service, etc.)
metadata: resource name and labels (labels uniquely identify pods or services)
spec: specification details such as replicas, container specs, ports, environment variables, and volumes
Deployment manifests separate deployment-level labels and specs from pod-level templates and labels. Pods use labels for service discovery.
Services specify ports, selectors (matching pod labels), and the type of service (ClusterIP, NodePort, LoadBalancer). Pods communicate through Services, not directly.
To apply a manifest, use:
kubectl apply -f manifest.yaml

* Kubernetes Networking
ClusterIP services are accessible only within the cluster.
To access pods from EC2 instances within the same VPC but a different subnet, use NodePort, which exposes pods on static ports on nodes.
To expose services externally, use LoadBalancer services, which create cloud provider load balancers (like AWS ELB).
Pods communicate with each other via services, not directly by IP because pod IPs can change.
